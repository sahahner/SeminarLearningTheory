## Chapters 1 (Introduction) and 2 (A Gentle Start)

##### Presenter: Michael Kamp

### Summary:

### Questions:
1. When calculating the error of an hypothesis why do we compare the error of the obtained hypothesis with $h^{*}$ (in hypothesis space $\mathcal{H}$) and not with $f$?
In general we compare results with $h^{*}$, because given the hypothesis space $\mathcal{H}$ it is the result closest to the real function one could possibly obtain.
2. Is it possible to calculate $h^{*}$?
3. In the sample set $S$ are duplicates of instances allowed? And what is the actual influence of duplicats in sample $S$?
Yes, duplicates are allowed because the instances are i.i.d. and therefore it is possible.
In practical apoaches duplicates in the sample set are very rare. Nevertheless, the algorithm should be prepared for this case since some methods could break, for example an inversion of a matrix because of singularity.
4. In the proof of the Corollary where do we use the assumption on realizability?
We utilize the assumption when assuming there exists an hypothesis with $\mathcal{L}_{S}(h)=0$.
5. Why $\varepsilon>0$ and not $\varepsilon \in [0,1]$ since it is a probability?
Later on epsilon could be the value of a loss function and considering $\varepsilon>0$ one can apply the corollary.
6. Would anyone ever calculate m utilizing the corollary?
For legal bounds for example in medicine being able to calculate a probability of an error is very important. Of cause the realizability assumption is very strong. Later on we are going to see a similar corollary without the assumption.
7. Is the upper bound in the corollary a sharp bound?
Sometimes the considered bounds are right but generally one can only proof uper and lower bounds.
8. The goal utilizing ERM is always to minimize the risk oder the input space. Rare cases (with respect to distribution $\mathcal{D}$) are not considered because they have a very low probabilty but especially those rare cases (for example the corner case in autonomous driving) are the ones that are most interesting because of their strong impacts. How can the algorithms be good on those rare cases?
ERM sadly doesn't work very well for those cases. But there are practical solutions for example
- to overdistribute the rare examples by hand or
- change the loss function. A bad outcome is asigned a very high value by the error function.
9. Having a machine that is very conservative in risk taking and tends to overestimate errors could be problematic. So a quite philosophical question is what is better: overestimate or underestimate the error?
10. What is overfitting?
